
<!DOCTYPE html
  PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN">
<html xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd">
   <head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   
      <!--
This HTML is auto-generated from an M-file.
To make changes, update the M-file and republish this document.
      -->
      <title>multivariable_calc_demo</title>
      <meta name="generator" content="MATLAB 7.4">
      <meta name="date" content="2007-09-03">
      <meta name="m-file" content="multivariable_calc_demo"><style>

body {
  background-color: white;
  margin:10px;
}

h1 {
  color: #990000; 
  font-size: x-large;
}

h2 {
  color: #990000;
  font-size: medium;
}

/* Make the text shrink to fit narrow windows, but not stretch too far in 
wide windows. */ 
p,h1,h2,div.content div {
  max-width: 600px;
  /* Hack for IE6 */
  width: auto !important; width: 600px;
}

pre.codeinput {
  background: #EEEEEE;
  padding: 10px;
}
@media print {
  pre.codeinput {word-wrap:break-word; width:100%;}
} 

span.keyword {color: #0000FF}
span.comment {color: #228B22}
span.string {color: #A020F0}
span.untermstring {color: #B20000}
span.syscmd {color: #B28C00}

pre.codeoutput {
  color: #666666;
  padding: 10px;
}

pre.error {
  color: red;
}

p.footer {
  text-align: right;
  font-size: xx-small;
  font-weight: lighter;
  font-style: italic;
  color: gray;
}

  </style></head>
   <body>
      <div class="content">
         <h2>Contents</h2>
         <div>
            <ul>
               <li><a href="#2">Gradient of the Rosenbrock function at [1,1], the global minimizer</a></li>
               <li><a href="#3">The Hessian matrix at the minimizer should be positive definite</a></li>
               <li><a href="#4">Gradient estimation using gradest - a function of 5 variables</a></li>
               <li><a href="#5">Simple Hessian matrix of a problem with 3 independent variables</a></li>
               <li><a href="#6">A semi-definite Hessian matrix</a></li>
               <li><a href="#7">Directional derivative of the Rosenbrock function at the solution</a></li>
               <li><a href="#8">Directional derivative at other locations</a></li>
               <li><a href="#9">Jacobian matrix of a scalar function is just the gradient</a></li>
               <li><a href="#10">Jacobian matrix of a linear system will reduce to the design matrix</a></li>
               <li><a href="#11">The jacobian matrix of a nonlinear transformation of variables</a></li>
            </ul>
         </div><pre class="codeinput"><span class="comment">% Multivariate calculus demo script</span>

<span class="comment">% This script file is designed to be used in cell mode</span>
<span class="comment">% from the matlab editor, or best of all, use the publish</span>
<span class="comment">% to HTML feature from the matlab editor. Older versions</span>
<span class="comment">% of matlab can copy and paste entire blocks of code into</span>
<span class="comment">% the Matlab command window.</span>

<span class="comment">% Typical usage of the gradient and Hessian might be in</span>
<span class="comment">% optimization problems, where one might compare an analytically</span>
<span class="comment">% derived gradient for correctness, or use the Hessian matrix</span>
<span class="comment">% to compute confidence interval estimates on parameters in a</span>
<span class="comment">% maximum likelihood estimation.</span>
</pre><h2>Gradient of the Rosenbrock function at [1,1], the global minimizer<a name="2"></a></h2><pre class="codeinput">rosen = @(x) (1-x(1)).^2 + 105*(x(2)-x(1).^2).^2;
<span class="comment">% The gradient should be zero (within floating point noise)</span>
[grad,err] = gradest(rosen,[1 1])
</pre><pre class="codeoutput">grad =
   3.6989e-20            0
err =
   1.4545e-18            0
</pre><h2>The Hessian matrix at the minimizer should be positive definite<a name="3"></a></h2><pre class="codeinput">H = hessian(rosen,[1 1])
<span class="comment">% The eigenvalues of h should be positive</span>
eig(H)
</pre><pre class="codeoutput">H =
          842         -420
         -420          210
ans =
      0.39939
       1051.6
</pre><h2>Gradient estimation using gradest - a function of 5 variables<a name="4"></a></h2><pre class="codeinput">[grad,err] = gradest(@(x) sum(x.^2),[1 2 3 4 5])
</pre><pre class="codeoutput">grad =
            2            4            6            8           10
err =
   1.2533e-14   2.5067e-14   3.4734e-14   5.0134e-14    2.836e-14
</pre><h2>Simple Hessian matrix of a problem with 3 independent variables<a name="5"></a></h2><pre class="codeinput">[H,err] = hessian(@(x) x(1) + x(2)^2 + x(3)^3,[1 2 3])
</pre><pre class="codeoutput">H =
     0     0     0
     0     2     0
     0     0    18
err =
            0            0            0
            0   4.6563e-15            0
            0            0   3.3318e-14
</pre><h2>A semi-definite Hessian matrix<a name="6"></a></h2><pre class="codeinput">H = hessian(@(xy) cos(xy(1) - xy(2)),[0 0])
<span class="comment">% one of these eigenvalues will be zero (approximately)</span>
eig(H)
</pre><pre class="codeoutput">H =
           -1            1
            1           -1
ans =
           -2
  -3.8795e-07
</pre><h2>Directional derivative of the Rosenbrock function at the solution<a name="7"></a></h2>
         <p>This should be zero. Ok, its a trivial test case.</p><pre class="codeinput">[dd,err] = directionaldiff(rosen,[1 1],[1 2])
</pre><pre class="codeoutput">dd =
     0
err =
     0
</pre><h2>Directional derivative at other locations<a name="8"></a></h2><pre class="codeinput">[dd,err] = directionaldiff(rosen,[2 3],[1 -1])

<span class="comment">% We can test this example</span>
v = [1 -1];
v = v/norm(v);
g = gradest(rosen,[2 3]);

<span class="comment">% The directional derivative will be the dot product of the gradient with</span>
<span class="comment">% the (unit normalized) vector. So this difference will be (approx) zero.</span>
dot(g,v) - dd
</pre><pre class="codeoutput">dd =
       743.88
err =
   1.5078e-12
ans =
   1.5916e-12
</pre><h2>Jacobian matrix of a scalar function is just the gradient<a name="9"></a></h2><pre class="codeinput">[jac,err] = jacobianest(rosen,[2 3])

grad = gradest(rosen,[2 3])
</pre><pre class="codeoutput">jac =
          842         -210
err =
   2.8698e-12   1.0642e-12
grad =
          842         -210
</pre><h2>Jacobian matrix of a linear system will reduce to the design matrix<a name="10"></a></h2><pre class="codeinput">A = rand(5,3);
b = rand(5,1);
fun = @(x) (A*x-b);

x = rand(3,1);
[jac,err] = jacobianest(fun,x)

disp <span class="string">'This should be essentially zero at any location x'</span>
jac - A
</pre><pre class="codeoutput">jac =
      0.81472      0.09754      0.15761
      0.90579       0.2785      0.97059
      0.12699      0.54688      0.95717
      0.91338      0.95751      0.48538
      0.63236      0.96489      0.80028
err =
   3.9634e-15   3.8376e-16   5.4271e-16
   3.9634e-15   8.8625e-16   5.0134e-15
   7.0064e-16   3.0701e-15   5.3175e-15
     3.76e-15   4.8542e-15   2.4271e-15
   3.0701e-15   4.3417e-15   3.9634e-15
This should be essentially zero at any location x
ans =
            0  -3.1919e-16            0
   1.1102e-16  -1.6653e-16            0
   2.7756e-17            0  -1.1102e-16
  -1.1102e-16  -3.3307e-16   1.6653e-16
            0   2.2204e-16   1.1102e-16
</pre><h2>The jacobian matrix of a nonlinear transformation of variables<a name="11"></a></h2>
         <p>evaluated at some arbitrary location [-2, -3]</p><pre class="codeinput">fun = @(xy) [xy(1).^2, cos(xy(1) - xy(2))];
[jac,err] = jacobianest(fun,[-2 -3])
</pre><pre class="codeoutput">jac =
           -4            0
     -0.84147      0.84147
err =
   2.5067e-14            0
   6.1478e-14   1.9658e-14
</pre><p class="footer"><br>
            Published with MATLAB&reg; 7.4<br></p>
      </div>
      <!--
##### SOURCE BEGIN #####
% Multivariate calculus demo script

% This script file is designed to be used in cell mode
% from the matlab editor, or best of all, use the publish
% to HTML feature from the matlab editor. Older versions
% of matlab can copy and paste entire blocks of code into
% the Matlab command window.

% Typical usage of the gradient and Hessian might be in
% optimization problems, where one might compare an analytically
% derived gradient for correctness, or use the Hessian matrix
% to compute confidence interval estimates on parameters in a
% maximum likelihood estimation.

%% Gradient of the Rosenbrock function at [1,1], the global minimizer
rosen = @(x) (1-x(1)).^2 + 105*(x(2)-x(1).^2).^2;
% The gradient should be zero (within floating point noise)
[grad,err] = gradest(rosen,[1 1])

%% The Hessian matrix at the minimizer should be positive definite
H = hessian(rosen,[1 1])
% The eigenvalues of h should be positive
eig(H)

%% Gradient estimation using gradest - a function of 5 variables
[grad,err] = gradest(@(x) sum(x.^2),[1 2 3 4 5])

%% Simple Hessian matrix of a problem with 3 independent variables
[H,err] = hessian(@(x) x(1) + x(2)^2 + x(3)^3,[1 2 3])

%% A semi-definite Hessian matrix
H = hessian(@(xy) cos(xy(1) - xy(2)),[0 0])
% one of these eigenvalues will be zero (approximately)
eig(H)

%% Directional derivative of the Rosenbrock function at the solution
% This should be zero. Ok, its a trivial test case.
[dd,err] = directionaldiff(rosen,[1 1],[1 2])

%% Directional derivative at other locations
[dd,err] = directionaldiff(rosen,[2 3],[1 -1])

% We can test this example
v = [1 -1];
v = v/norm(v);
g = gradest(rosen,[2 3]);

% The directional derivative will be the dot product of the gradient with
% the (unit normalized) vector. So this difference will be (approx) zero.
dot(g,v) - dd

%% Jacobian matrix of a scalar function is just the gradient
[jac,err] = jacobianest(rosen,[2 3])

grad = gradest(rosen,[2 3])

%% Jacobian matrix of a linear system will reduce to the design matrix
A = rand(5,3);
b = rand(5,1);
fun = @(x) (A*x-b);

x = rand(3,1);
[jac,err] = jacobianest(fun,x)

disp 'This should be essentially zero at any location x'
jac - A

%% The jacobian matrix of a nonlinear transformation of variables
% evaluated at some arbitrary location [-2, -3]
fun = @(xy) [xy(1).^2, cos(xy(1) - xy(2))];
[jac,err] = jacobianest(fun,[-2 -3])



##### SOURCE END #####
-->
   </body>
</html>